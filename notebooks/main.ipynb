{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b1abb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random as rd\n",
    "from pathlib import Path\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import ResNet50_Weights, ViT_B_16_Weights\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "import json\n",
    "\n",
    "\n",
    "# Enable inline plotting for Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# Set device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bebf32",
   "metadata": {},
   "source": [
    "# utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23b44f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(results, model_name):\n",
    "    \"\"\"\n",
    "    Plots training and validation loss/accuracy curves for Deep Learning models.\n",
    "    \"\"\"\n",
    "    train_loss = results['train_loss']\n",
    "    val_loss = results['val_loss']\n",
    "\n",
    "    train_acc = results['train_acc']\n",
    "    val_acc = results['val_acc']\n",
    "\n",
    "    epochs = range(len(results['train_loss']))\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    \n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, label='Train Loss')\n",
    "    plt.plot(epochs, val_loss, label='Val Loss')\n",
    "    plt.title(f'{model_name} - Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_acc, label='Train Accuracy')\n",
    "    plt.plot(epochs, val_acc, label='Val Accuracy')\n",
    "    plt.title(f'{model_name} - Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    if not os.path.exists(\"plots\"):\n",
    "        os.makedirs(\"plots\")\n",
    "\n",
    "    save_path = f\"plots/{model_name}_curves.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close() # Close plot to save memory\n",
    "    # plt.show() # Uncomment if you want to see plots immediately in notebook\n",
    "\n",
    "def plot_mlp_loss_curve(mlp_model, model_name=\"MLP\"):\n",
    "    \"\"\"\n",
    "    Plots the loss curve for Scikit-Learn MLPClassifier.\n",
    "    \"\"\"\n",
    "    if not hasattr(mlp_model, 'loss_curve_'):\n",
    "        print(f\"Warning: {model_name} does not have a loss curve.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(mlp_model.loss_curve_, label='Training Loss')\n",
    "    plt.title(f'{model_name} - Training Loss Curve')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    if not os.path.exists(\"plots\"):\n",
    "        os.makedirs(\"plots\")\n",
    "\n",
    "    save_path = f\"plots/{model_name}_loss_curve.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, model_name):\n",
    "    \"\"\"\n",
    "    Plots confusion matrix for both DL and ML models.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    \n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted')\n",
    "\n",
    "    if not os.path.exists(\"plots\"):\n",
    "        os.makedirs(\"plots\")\n",
    "    \n",
    "    save_path = f\"plots/{model_name}_conf_matrix.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb75af2c",
   "metadata": {},
   "source": [
    "# data_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29d0a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(data_dir, batch_size=32, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Loads and splits the dataset. Returns the dataloaders\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(data_dir):\n",
    "        raise FileNotFoundError(f\"Data directory not found: {data_dir}\")\n",
    "\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    full_dataset = datasets.ImageFolder(\n",
    "                    root=data_dir,\n",
    "                    transform=data_transform\n",
    "                )\n",
    "\n",
    "    class_names = full_dataset.classes\n",
    "    print(f\"Classes found: {class_names}\")\n",
    "\n",
    "    indices_by_writer = {}\n",
    "    all_writers = set()\n",
    "\n",
    "    for idx, (path, label) in enumerate(full_dataset.samples):\n",
    "        filename = Path(path).name\n",
    "        filename_no_ext = os.path.splitext(filename)[0]\n",
    "\n",
    "        parts = filename_no_ext.split('_')\n",
    "\n",
    "        if len(parts) >= 4:\n",
    "            writer_id = parts[-2]  #Examle: Fire_brigade_00000_1_12 -> parts[-2] = \"1\"\n",
    "        \n",
    "        else:\n",
    "            print(f\"Wrong file format: {filename}\")\n",
    "            writer_id = \"unknown\"\n",
    "\n",
    "        if writer_id not in indices_by_writer:\n",
    "            indices_by_writer[writer_id] = []\n",
    "\n",
    "        indices_by_writer[writer_id].append(idx)\n",
    "        all_writers.add(writer_id)\n",
    "\n",
    "    writers_list = sorted(list(all_writers))\n",
    "    rd.seed(42)\n",
    "    rd.shuffle(writers_list)\n",
    "\n",
    "    total_writers = len(writers_list)\n",
    "    n_train = int(total_writers * train_ratio)\n",
    "    n_val = int(total_writers * val_ratio)\n",
    "\n",
    "    train_writers = writers_list[:n_train]\n",
    "    val_writers = writers_list[n_train : n_train + n_val]\n",
    "    test_writers = writers_list[n_train + n_val:]\n",
    "\n",
    "    print(f\"Total participants: {total_writers}\")\n",
    "    print(f\"Training participants: ({len(train_writers)}): {train_writers[:5]}...\")\n",
    "    print(f\"Testing participants: ({len(test_writers)}): {test_writers}...\")\n",
    "\n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    for w in train_writers:\n",
    "        train_indices.extend(indices_by_writer[w])\n",
    "    for w in val_writers:\n",
    "        val_indices.extend(indices_by_writer[w])\n",
    "    for w in test_writers:\n",
    "        test_indices.extend(indices_by_writer[w])\n",
    "    \n",
    "    train_data = Subset(full_dataset, train_indices)\n",
    "    val_data = Subset(full_dataset, val_indices)\n",
    "    test_data = Subset(full_dataset, test_indices)\n",
    "\n",
    "    print(f\"Sample Sizes -> Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db152d8",
   "metadata": {},
   "source": [
    "# model_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05be5a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, num_classes, device):\n",
    "    \"\"\"\n",
    "    Gets the wanted model from torch and returns it\n",
    "    \"\"\"\n",
    "\n",
    "    if model_name == \"resnet50\":\n",
    "        weights = ResNet50_Weights.DEFAULT\n",
    "        model = models.resnet50(weights=weights)\n",
    "\n",
    "        num_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    elif model_name == \"vit_b_16\":\n",
    "        weights = ViT_B_16_Weights.DEFAULT\n",
    "        model = models.vit_b_16(weights=weights)\n",
    "\n",
    "        num_features = model.heads.head.in_features\n",
    "        model.heads.head = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    else:\n",
    "        print(\"Error. Wrong model name\")\n",
    "        return None\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd486492",
   "metadata": {},
   "source": [
    "# engine.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1828eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"\"Train the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_preds += torch.sum(preds == labels.data)\n",
    "        total_preds += labels.size(0)\n",
    "\n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    epoch_acc = correct_preds.double() / total_preds\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate the model for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            correct_preds += torch.sum(preds == labels.data)\n",
    "            total_preds += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total_preds\n",
    "    epoch_acc = correct_preds.double() / total_preds\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def test(model, dataloader, device, class_names):\n",
    "    \"\"\"Test the model and print classification report.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(classification_report(\n",
    "            all_labels,\n",
    "            all_preds,\n",
    "            target_names=class_names,\n",
    "            labels=np.arange(len(class_names))\n",
    "        ))\n",
    "\n",
    "    return all_labels, all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba78369",
   "metadata": {},
   "source": [
    "# ML Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0004da85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IconMLManager:\n",
    "    \"\"\"Does data loading and model training for icon features\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def features_to_vectors(dataset):\n",
    "        \"\"\"Flatten JSON structure into a matrix X, label matrix y, and group matrix (writer_id)\"\"\"\n",
    "        _x = []\n",
    "        _y = []\n",
    "        _groups = []  # List to store writer IDs for GroupShuffleSplit\n",
    "\n",
    "        for sample in dataset:\n",
    "            feature_vector = []\n",
    "\n",
    "            # Process subdivisions\n",
    "            for sub in sample['subdivisions']:\n",
    "                feature_vector.extend([\n",
    "                    sub['perimeter'],\n",
    "                    sub['area'],\n",
    "                    sub['compactness'],\n",
    "                    sub['corners_count'],\n",
    "                    sub['sharp_corners_count']\n",
    "                ])\n",
    "                feature_vector.extend(sub['hu_moments'])\n",
    "                feature_vector.extend([\n",
    "                    sub['line_directions']['horizontal'],\n",
    "                    sub['line_directions']['vertical'],\n",
    "                    sub['line_directions']['diag1'],\n",
    "                    sub['line_directions']['diag2']\n",
    "                ])\n",
    "\n",
    "            # Process global features\n",
    "            g = sample['global']\n",
    "            feature_vector.extend([\n",
    "                g['perimeter'],\n",
    "                g['area'],\n",
    "                g['compactness'],\n",
    "                g['corners_count'],\n",
    "                g['sharp_corners_count'],\n",
    "                g['ellipse_count'],\n",
    "                g['diagonal_length'],\n",
    "                g['diagonal_angle'],\n",
    "                g['convex_area']['convex_area'],\n",
    "                g['convex_area']['solidity'],\n",
    "                g['avg_centroidal_radius']\n",
    "            ])\n",
    "            feature_vector.extend(g['hu_moments'])\n",
    "            feature_vector.extend([\n",
    "                g['line_directions']['horizontal'],\n",
    "                g['line_directions']['vertical'],\n",
    "                g['line_directions']['diag1'],\n",
    "                g['line_directions']['diag2']\n",
    "            ])\n",
    "\n",
    "            _x.append(feature_vector)\n",
    "            _y.append(sample['label'])\n",
    "\n",
    "            \n",
    "            filename = sample.get('filename', '')  # Example: Fire_00000_1_05.png -> Writer ID is \"1\"\n",
    "            parts = filename.replace('.png', '').split('_')\n",
    "\n",
    "            if len(parts) >= 4:\n",
    "                # The writer ID is typically the second to last element\n",
    "                writer_id = parts[-2]\n",
    "            else:\n",
    "                writer_id = 'unknown'\n",
    "            \n",
    "            _groups.append(writer_id)\n",
    "\n",
    "\n",
    "        # Return groups along with X and y\n",
    "        return np.array(_x), np.array(_y), np.array(_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba6e8d6",
   "metadata": {},
   "source": [
    "# ML Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3cc12d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ml(json_path_str):\n",
    "    \"\"\"Runs the SVM and MLP training using extracted features with Writer-Independent Split\"\"\"\n",
    "    print(f\"\\n{'='*30}\")\n",
    "    print(\"STARTING TRADITIONAL ML PIPELINE (SVM & MLP)\")\n",
    "    print(f\"{'='*30}\")\n",
    "\n",
    "    json_path = Path(json_path_str)\n",
    "\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            raw_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {json_path} not found. Ensure extracted features exist.\")\n",
    "        return\n",
    "\n",
    "    ml_manager = IconMLManager()\n",
    "    X, y_raw, groups = ml_manager.features_to_vectors(raw_data)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y_raw)\n",
    "\n",
    "    print(f\"Dataset: {X.shape[0]} samples, {X.shape[1]} features per sample.\")\n",
    "    print(f\"Total Unique Writers: {len(np.unique(groups))}\")\n",
    "\n",
    "    # Split Train (70%) and Temp (30%)\n",
    "    gss_train = GroupShuffleSplit(n_splits=1, train_size=0.70, random_state=42)\n",
    "    train_idx, temp_idx = next(gss_train.split(X, y, groups))\n",
    "    \n",
    "    X_train, X_temp = X[train_idx], X[temp_idx]\n",
    "    y_train, y_temp = y[train_idx], y[temp_idx]\n",
    "    groups_temp = groups[temp_idx]\n",
    "\n",
    "    # Split Temp into Val (15%) and Test (15%)\n",
    "    gss_val = GroupShuffleSplit(n_splits=1, test_size=0.50, random_state=42)\n",
    "    val_idx, test_idx = next(gss_val.split(X_temp, y_temp, groups_temp))\n",
    "\n",
    "    X_val, X_test = X_temp[val_idx], X_temp[test_idx]\n",
    "    y_val, y_test = y_temp[val_idx], y_temp[test_idx]\n",
    "\n",
    "    print(f\"Split: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # SVM\n",
    "    print(\"\\n--- Training SVM ---\")\n",
    "    svm_model = SVC(kernel='rbf', C=10.0, gamma='scale', random_state=42)\n",
    "    svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    val_pred_svm = svm_model.predict(X_val_scaled)\n",
    "    test_pred_svm = svm_model.predict(X_test_scaled)\n",
    "    print(f\"SVM Val Accuracy: {accuracy_score(y_val, val_pred_svm):.4f}\")\n",
    "    print(f\"SVM Test Accuracy: {accuracy_score(y_test, test_pred_svm):.4f}\")\n",
    "\n",
    "    plot_confusion_matrix(y_test, test_pred_svm, le.classes_, \"SVM\")\n",
    "\n",
    "    # MLP\n",
    "    print(\"\\n --- Training MLP (Sklearn) ---\")\n",
    "    mlp_model = MLPClassifier(\n",
    "        hidden_layer_sizes=(128, 64),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=1000,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    mlp_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    val_pred_mlp = mlp_model.predict(X_val_scaled)\n",
    "    print(f\"MLP Val Accuracy: {accuracy_score(y_val, val_pred_mlp):.4f}\")\n",
    "\n",
    "    test_pred_mlp = mlp_model.predict(X_test_scaled)\n",
    "    print(f\"MLP Test Accuracy: {accuracy_score(y_test, test_pred_mlp):.4f}\")\n",
    "\n",
    "    print(\"\\n --- MLP Detailed Report ----\")\n",
    "    print(classification_report(y_test, test_pred_mlp, target_names=le.classes_))\n",
    "    \n",
    "    plot_confusion_matrix(y_test, test_pred_mlp, le.classes_, \"MLP\")\n",
    "    plot_mlp_loss_curve(mlp_model, \"MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b02f0c9",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02d94e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cuda\n",
      "\n",
      "==============================\n",
      "STARTING TRADITIONAL ML PIPELINE (SVM & MLP)\n",
      "==============================\n",
      "Dataset: 2095 samples, 118 features per sample.\n",
      "Total Unique Writers: 30\n",
      "Split: Train=1465, Val=280, Test=350\n",
      "\n",
      "--- Training SVM ---\n",
      "SVM Val Accuracy: 0.9321\n",
      "SVM Test Accuracy: 0.9400\n",
      "\n",
      " --- Training MLP (Sklearn) ---\n",
      "MLP Val Accuracy: 0.8893\n",
      "MLP Test Accuracy: 0.9000\n",
      "\n",
      " --- MLP Detailed Report ----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bomb       0.96      0.96      0.96        25\n",
      "         Car       0.86      1.00      0.93        25\n",
      "    Casualty       0.85      0.92      0.88        25\n",
      " Electricity       1.00      0.92      0.96        25\n",
      "        Fire       0.73      0.96      0.83        25\n",
      "Fire_brigade       1.00      0.96      0.98        25\n",
      "       Flood       0.89      0.64      0.74        25\n",
      "         Gas       0.85      0.92      0.88        25\n",
      "      Injury       0.91      0.84      0.88        25\n",
      "  Paramedics       0.92      0.92      0.92        25\n",
      "      Person       0.96      0.96      0.96        25\n",
      "      Police       0.96      0.92      0.94        25\n",
      "  Road_block       0.82      0.72      0.77        25\n",
      "     Warning       0.96      0.96      0.96        25\n",
      "\n",
      "    accuracy                           0.90       350\n",
      "   macro avg       0.91      0.90      0.90       350\n",
      "weighted avg       0.91      0.90      0.90       350\n",
      "\n",
      "Classes found: ['Bomb', 'Car', 'Casualty', 'Electricity', 'Fire', 'Fire_brigade', 'Flood', 'Gas', 'Injury', 'Paramedics', 'Person', 'Police', 'Road_block', 'Warning']\n",
      "Total participants: 30\n",
      "Training participants: (21): ['27', '22', '19', '6', '3']...\n",
      "Testing participants: (5): ['17', '30', '1', '12', '28']...\n",
      "Sample Sizes -> Train: 1465, Val: 280, Test: 350\n",
      "\n",
      "------------------------------\n",
      "NOW TRAINING resnet50\n",
      "\n",
      "------------------------------\n",
      "Epoch 1/10 | Train Loss: 1.5782 | Train Acc: 0.7270 | Val Loss: 0.2343 | Val Acc: 0.9607\n",
      "Epoch 2/10 | Train Loss: 0.0794 | Train Acc: 0.9891 | Val Loss: 0.0101 | Val Acc: 1.0000\n",
      "Epoch 3/10 | Train Loss: 0.0212 | Train Acc: 0.9973 | Val Loss: 0.0107 | Val Acc: 0.9964\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[32m     89\u001b[39m         torch.cuda.empty_cache()\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[43mmain_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mmain_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     45\u001b[39m results = {\n\u001b[32m     46\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m     47\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrain_acc\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m     48\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m     49\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mval_acc\u001b[39m\u001b[33m\"\u001b[39m: []\n\u001b[32m     50\u001b[39m }\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[32m     53\u001b[39m     \u001b[38;5;66;03m# train and validate functions (from previous cells)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     train_loss, train_acc = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     val_loss, val_acc = validate(model, val_loader, criterion, DEVICE)\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Tensor check and float conversion\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, device)\u001b[39m\n\u001b[32m     13\u001b[39m outputs = model(images)\n\u001b[32m     14\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m optimizer.step()\n\u001b[32m     19\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- GLOBAL SETTINGS ---\n",
    "# Accessing data by going up one directory since notebook is in \"notebooks\"\n",
    "DATA_PATH = \"../data/extracted\"\n",
    "JSON_FEATURES_PATH = \"../features_dataset.json\" \n",
    "\n",
    "MODELS_DL = [\"resnet50\", \"vit_b_16\"]\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.0001\n",
    "\n",
    "def main_pipeline():\n",
    "    print(f\"Working on {DEVICE}\")\n",
    "\n",
    "    # 1. TRADITIONAL ML PART\n",
    "\n",
    "    if Path(JSON_FEATURES_PATH).exists():\n",
    "        train_ml(JSON_FEATURES_PATH)\n",
    "    else:\n",
    "        print(f\"Warning: {JSON_FEATURES_PATH} not found. Skipping ML training.\")\n",
    "        print(\"Please run extraction.ipynb first to generate the JSON file.\")\n",
    "    \n",
    "    # 2. DEEP LEARNING PART\n",
    "\n",
    "    if not Path(DATA_PATH).exists():\n",
    "        print(f\"Error: {DATA_PATH} not found.\")\n",
    "        return\n",
    "\n",
    "    # Using create_dataloaders function defined in previous cells\n",
    "    train_loader, val_loader, test_loader, class_names = create_dataloaders(DATA_PATH, BATCH_SIZE)\n",
    "\n",
    "    if not os.path.exists(\"models\"):\n",
    "        os.makedirs(\"models\")\n",
    "\n",
    "    for model_name in MODELS_DL:\n",
    "        print(f\"\\n{'-'*30}\")\n",
    "        print(f\"NOW TRAINING {model_name}\")\n",
    "        print(f\"\\n{'-'*30}\")\n",
    "\n",
    "        # get_model function (from previous cells)\n",
    "        model = get_model(model_name, len(class_names), DEVICE)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "        results = {\n",
    "            \"train_loss\": [],\n",
    "            \"train_acc\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"val_acc\": []\n",
    "        }\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            # train and validate functions (from previous cells)\n",
    "            train_loss, train_acc = train(model, train_loader, criterion, optimizer, DEVICE)\n",
    "            val_loss, val_acc = validate(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "            # Tensor check and float conversion\n",
    "            t_acc = train_acc.item() if isinstance(train_acc, torch.Tensor) else train_acc\n",
    "            v_acc = val_acc.item() if isinstance(val_acc, torch.Tensor) else val_acc\n",
    "\n",
    "            results[\"train_loss\"].append(train_loss)\n",
    "            results[\"train_acc\"].append(t_acc)\n",
    "            results[\"val_loss\"].append(val_loss)\n",
    "            results[\"val_acc\"].append(v_acc)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "                f\"Train Loss: {train_loss:.4f} | \"\n",
    "                f\"Train Acc: {t_acc:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} | \"\n",
    "                f\"Val Acc: {v_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "        print(f\"\\n--- {model_name} TEST RESULTS ---\")\n",
    "\n",
    "        # test function (from previous cells)\n",
    "        y_true, y_pred = test(model, test_loader, DEVICE, class_names)\n",
    "\n",
    "        # Visualization (using utils functions)\n",
    "        plot_curves(results, model_name)\n",
    "        plot_confusion_matrix(y_true, y_pred, class_names, model_name)\n",
    "\n",
    "        # Saving the Model\n",
    "        torch.save(model.state_dict(), f\"models/{model_name}_final.pth\")\n",
    "        print(\"Model Saved\")\n",
    "\n",
    "        # Memory cleanup\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
