{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b1abb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random as rd\n",
    "from pathlib import Path\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import ResNet50_Weights, ViT_B_16_Weights, ResNet18_Weights\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import json\n",
    "\n",
    "# Enable inline plotting for Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# Set device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bebf32",
   "metadata": {},
   "source": [
    "# utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23b44f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(results, model_name):\n",
    "    \"\"\"\n",
    "    Plots training and validation loss/accuracy curves for Deep Learning models.\n",
    "    \"\"\"\n",
    "    train_loss = results['train_loss']\n",
    "    val_loss = results['val_loss']\n",
    "\n",
    "    train_acc = results['train_acc']\n",
    "    val_acc = results['val_acc']\n",
    "\n",
    "    epochs = range(len(results['train_loss']))\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    \n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, label='Train Loss')\n",
    "    plt.plot(epochs, val_loss, label='Val Loss')\n",
    "    plt.title(f'{model_name} - Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_acc, label='Train Accuracy')\n",
    "    plt.plot(epochs, val_acc, label='Val Accuracy')\n",
    "    plt.title(f'{model_name} - Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    if not os.path.exists(\"plots\"):\n",
    "        os.makedirs(\"plots\")\n",
    "\n",
    "    save_path = f\"plots/{model_name}_curves.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close() \n",
    "    # plt.show() \n",
    "\n",
    "def plot_mlp_loss_curve(mlp_model, model_name=\"MLP\"):\n",
    "    \"\"\"\n",
    "    Plots the loss curve for Scikit-Learn MLPClassifier.\n",
    "    \"\"\"\n",
    "    if not hasattr(mlp_model, 'loss_curve_'):\n",
    "        print(f\"Warning: {model_name} does not have a loss curve.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(mlp_model.loss_curve_, label='Training Loss')\n",
    "    plt.title(f'{model_name} - Training Loss Curve')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    if not os.path.exists(\"plots\"):\n",
    "        os.makedirs(\"plots\")\n",
    "\n",
    "    save_path = f\"plots/{model_name}_loss_curve.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, model_name):\n",
    "    \"\"\"\n",
    "    Plots confusion matrix for both DL and ML models.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    \n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted')\n",
    "\n",
    "    if not os.path.exists(\"plots\"):\n",
    "        os.makedirs(\"plots\")\n",
    "    \n",
    "    save_path = f\"plots/{model_name}_conf_matrix.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb75af2c",
   "metadata": {},
   "source": [
    "# data_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29d0a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformedDatasetWrapper(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.subset[index]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "def create_dataloaders(data_dir, batch_size=32, train_ratio=0.7, val_ratio=0.15):\n",
    "    if not os.path.exists(data_dir):\n",
    "        raise FileNotFoundError(f\"Data directory not found: {data_dir}\")\n",
    "\n",
    "    # Augmentation for training\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Standard transform for val/test (No augmentation)\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Load dataset without transforms initially\n",
    "    full_dataset = datasets.ImageFolder(root=data_dir, transform=None)\n",
    "    class_names = full_dataset.classes\n",
    "    print(f\"Classes found: {class_names}\")\n",
    "\n",
    "    # Split by Writer ID\n",
    "    indices_by_writer = {}\n",
    "    all_writers = set()\n",
    "\n",
    "    for idx, (path, label) in enumerate(full_dataset.samples):\n",
    "        filename = Path(path).name\n",
    "        filename_no_ext = os.path.splitext(filename)[0]\n",
    "        parts = filename_no_ext.split('_')\n",
    "\n",
    "        if len(parts) >= 4:\n",
    "            writer_id = parts[-2]\n",
    "        else:\n",
    "            print(f\"Wrong file format: {filename}\")\n",
    "            writer_id = \"unknown\"\n",
    "\n",
    "        if writer_id not in indices_by_writer:\n",
    "            indices_by_writer[writer_id] = []\n",
    "\n",
    "        indices_by_writer[writer_id].append(idx)\n",
    "        all_writers.add(writer_id)\n",
    "\n",
    "    writers_list = sorted(list(all_writers))\n",
    "    rd.seed(42)\n",
    "    rd.shuffle(writers_list)\n",
    "\n",
    "    total_writers = len(writers_list)\n",
    "    n_train = int(total_writers * train_ratio)\n",
    "    n_val = int(total_writers * val_ratio)\n",
    "\n",
    "    train_writers = writers_list[:n_train]\n",
    "    val_writers = writers_list[n_train : n_train + n_val]\n",
    "    test_writers = writers_list[n_train + n_val:]\n",
    "\n",
    "    print(f\"Total participants: {total_writers}\")\n",
    "    print(f\"Training participants ({len(train_writers)}): {train_writers[:5]}...\")\n",
    "    print(f\"Testing participants ({len(test_writers)}): {test_writers}...\")\n",
    "\n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    for w in train_writers:\n",
    "        train_indices.extend(indices_by_writer[w])\n",
    "    for w in val_writers:\n",
    "        val_indices.extend(indices_by_writer[w])\n",
    "    for w in test_writers:\n",
    "        test_indices.extend(indices_by_writer[w])\n",
    "    \n",
    "    # Apply transforms using Wrapper\n",
    "    train_data = TransformedDatasetWrapper(Subset(full_dataset, train_indices), transform=train_transform)\n",
    "    val_data = TransformedDatasetWrapper(Subset(full_dataset, val_indices), transform=test_transform)\n",
    "    test_data = TransformedDatasetWrapper(Subset(full_dataset, test_indices), transform=test_transform)\n",
    "\n",
    "    print(f\"Sample Sizes -> Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "    # Create Loaders\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db152d8",
   "metadata": {},
   "source": [
    "# model_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05be5a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 28 * 28, 512), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5), \n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def get_model(model_name, num_classes, device):\n",
    "    \"\"\"\n",
    "    Gets the wanted model from torch and returns it\n",
    "    \"\"\"\n",
    "\n",
    "    if model_name == \"resnet50\":\n",
    "        weights = ResNet50_Weights.DEFAULT\n",
    "        model = models.resnet50(weights=weights)\n",
    "\n",
    "        num_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    elif model_name == \"resnet18\":\n",
    "        weights = ResNet18_Weights.DEFAULT\n",
    "        model = models.resnet18(weights=weights)\n",
    "\n",
    "        num_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    elif model_name == \"vit_b_16\":\n",
    "        weights = ViT_B_16_Weights.DEFAULT\n",
    "        model = models.vit_b_16(weights=weights)\n",
    "\n",
    "        num_features = model.heads.head.in_features\n",
    "        model.heads.head = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    elif model_name == \"simple_cnn\":\n",
    "        model = SimpleCNN(num_classes)\n",
    "\n",
    "    else:\n",
    "        print(\"Error. Wrong model name\")\n",
    "        return None\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd486492",
   "metadata": {},
   "source": [
    "# engine.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1828eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"\"Train the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_preds += torch.sum(preds == labels.data)\n",
    "        total_preds += labels.size(0)\n",
    "\n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    epoch_acc = correct_preds.double() / total_preds\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate the model for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            correct_preds += torch.sum(preds == labels.data)\n",
    "            total_preds += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total_preds\n",
    "    epoch_acc = correct_preds.double() / total_preds\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def test(model, dataloader, device, class_names):\n",
    "    \"\"\"Test the model and print classification report.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(classification_report(\n",
    "            all_labels,\n",
    "            all_preds,\n",
    "            target_names=class_names,\n",
    "            labels=np.arange(len(class_names)),\n",
    "            zero_division=0\n",
    "        ))\n",
    "\n",
    "    return all_labels, all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba78369",
   "metadata": {},
   "source": [
    "# ML Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0004da85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IconMLManager:\n",
    "    \"\"\"Does data loading and model training for icon features\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def features_to_vectors(dataset):\n",
    "        \"\"\"Flatten JSON structure into a matrix X, label matrix y, and group matrix (writer_id)\"\"\"\n",
    "        _x = []\n",
    "        _y = []\n",
    "        _groups = []  # List to store writer IDs for GroupShuffleSplit\n",
    "\n",
    "        for sample in dataset:\n",
    "            feature_vector = []\n",
    "\n",
    "            # Process subdivisions\n",
    "            for sub in sample['subdivisions']:\n",
    "                feature_vector.extend([\n",
    "                    sub['perimeter'],\n",
    "                    sub['area'],\n",
    "                    sub['compactness'],\n",
    "                    sub['corners_count'],\n",
    "                    sub['sharp_corners_count']\n",
    "                ])\n",
    "                feature_vector.extend(sub['hu_moments'])\n",
    "                feature_vector.extend([\n",
    "                    sub['line_directions']['horizontal'],\n",
    "                    sub['line_directions']['vertical'],\n",
    "                    sub['line_directions']['diag1'],\n",
    "                    sub['line_directions']['diag2']\n",
    "                ])\n",
    "\n",
    "            # Process global features\n",
    "            g = sample['global']\n",
    "            feature_vector.extend([\n",
    "                g['perimeter'],\n",
    "                g['area'],\n",
    "                g['compactness'],\n",
    "                g['corners_count'],\n",
    "                g['sharp_corners_count'],\n",
    "                g['ellipse_count'],\n",
    "                g['diagonal_length'],\n",
    "                g['diagonal_angle'],\n",
    "                g['convex_area']['convex_area'],\n",
    "                g['convex_area']['solidity'],\n",
    "                g['avg_centroidal_radius']\n",
    "            ])\n",
    "            feature_vector.extend(g['hu_moments'])\n",
    "            feature_vector.extend([\n",
    "                g['line_directions']['horizontal'],\n",
    "                g['line_directions']['vertical'],\n",
    "                g['line_directions']['diag1'],\n",
    "                g['line_directions']['diag2']\n",
    "            ])\n",
    "\n",
    "            _x.append(feature_vector)\n",
    "            _y.append(sample['label'])\n",
    "\n",
    "            \n",
    "            filename = sample.get('filename', '')  # Example: Fire_00000_1_05.png -> Writer ID is \"1\"\n",
    "            parts = filename.replace('.png', '').split('_')\n",
    "\n",
    "            if len(parts) >= 4:\n",
    "                # The writer ID is typically the second to last element\n",
    "                writer_id = parts[-2]\n",
    "            else:\n",
    "                writer_id = 'unknown'\n",
    "            \n",
    "            _groups.append(writer_id)\n",
    "\n",
    "\n",
    "        # Return groups along with X and y\n",
    "        return np.array(_x), np.array(_y), np.array(_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba6e8d6",
   "metadata": {},
   "source": [
    "# ML Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3cc12d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ml(json_path_str):\n",
    "    \"\"\"Runs the SVM and MLP training using extracted features with Writer-Independent Split\"\"\"\n",
    "    print(f\"\\n{'='*30}\")\n",
    "    print(\"STARTING TRADITIONAL ML PIPELINE (SVM & MLP)\")\n",
    "    print(f\"{'='*30}\")\n",
    "\n",
    "    json_path = Path(json_path_str)\n",
    "\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            raw_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {json_path} not found. Ensure extracted features exist.\")\n",
    "        return\n",
    "\n",
    "    ml_manager = IconMLManager()\n",
    "    X, y_raw, groups = ml_manager.features_to_vectors(raw_data)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y_raw)\n",
    "\n",
    "    print(f\"Dataset: {X.shape[0]} samples, {X.shape[1]} features per sample.\")\n",
    "    print(f\"Total Unique Writers: {len(np.unique(groups))}\")\n",
    "\n",
    "    # Split Train (70%) and Temp (30%)\n",
    "    gss_train = GroupShuffleSplit(n_splits=1, train_size=0.70, random_state=42)\n",
    "    train_idx, temp_idx = next(gss_train.split(X, y, groups))\n",
    "    \n",
    "    X_train, X_temp = X[train_idx], X[temp_idx]\n",
    "    y_train, y_temp = y[train_idx], y[temp_idx]\n",
    "    groups_temp = groups[temp_idx]\n",
    "\n",
    "    # Split Temp into Val (15%) and Test (15%)\n",
    "    gss_val = GroupShuffleSplit(n_splits=1, test_size=0.50, random_state=42)\n",
    "    val_idx, test_idx = next(gss_val.split(X_temp, y_temp, groups_temp))\n",
    "\n",
    "    X_val, X_test = X_temp[val_idx], X_temp[test_idx]\n",
    "    y_val, y_test = y_temp[val_idx], y_temp[test_idx]\n",
    "\n",
    "    print(f\"Split: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # SVM\n",
    "    print(\"\\n--- Training SVM ---\")\n",
    "    svm_model = SVC(kernel='rbf', C=10.0, gamma='scale', random_state=42)\n",
    "    svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    val_pred_svm = svm_model.predict(X_val_scaled)\n",
    "    test_pred_svm = svm_model.predict(X_test_scaled)\n",
    "    print(f\"SVM Val Accuracy: {accuracy_score(y_val, val_pred_svm):.4f}\")\n",
    "    print(f\"SVM Test Accuracy: {accuracy_score(y_test, test_pred_svm):.4f}\")\n",
    "\n",
    "    plot_confusion_matrix(y_test, test_pred_svm, le.classes_, \"SVM\")\n",
    "\n",
    "    # MLP\n",
    "    print(\"\\n --- Training MLP (Sklearn) ---\")\n",
    "    mlp_model = MLPClassifier(\n",
    "        hidden_layer_sizes=(128, 64),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=1000,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    mlp_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    val_pred_mlp = mlp_model.predict(X_val_scaled)\n",
    "    print(f\"MLP Val Accuracy: {accuracy_score(y_val, val_pred_mlp):.4f}\")\n",
    "\n",
    "    test_pred_mlp = mlp_model.predict(X_test_scaled)\n",
    "    print(f\"MLP Test Accuracy: {accuracy_score(y_test, test_pred_mlp):.4f}\")\n",
    "\n",
    "    print(\"\\n --- MLP Detailed Report ----\")\n",
    "    print(classification_report(y_test, test_pred_mlp, target_names=le.classes_))\n",
    "    \n",
    "    plot_confusion_matrix(y_test, test_pred_mlp, le.classes_, \"MLP\")\n",
    "    plot_mlp_loss_curve(mlp_model, \"MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b02f0c9",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02d94e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cuda\n",
      "\n",
      "==============================\n",
      "STARTING TRADITIONAL ML PIPELINE (SVM & MLP)\n",
      "==============================\n",
      "Dataset: 2095 samples, 118 features per sample.\n",
      "Total Unique Writers: 30\n",
      "Split: Train=1465, Val=280, Test=350\n",
      "\n",
      "--- Training SVM ---\n",
      "SVM Val Accuracy: 0.9321\n",
      "SVM Test Accuracy: 0.9400\n",
      "\n",
      " --- Training MLP (Sklearn) ---\n",
      "MLP Val Accuracy: 0.8893\n",
      "MLP Test Accuracy: 0.9000\n",
      "\n",
      " --- MLP Detailed Report ----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bomb       0.96      0.96      0.96        25\n",
      "         Car       0.86      1.00      0.93        25\n",
      "    Casualty       0.85      0.92      0.88        25\n",
      " Electricity       1.00      0.92      0.96        25\n",
      "        Fire       0.73      0.96      0.83        25\n",
      "Fire_brigade       1.00      0.96      0.98        25\n",
      "       Flood       0.89      0.64      0.74        25\n",
      "         Gas       0.85      0.92      0.88        25\n",
      "      Injury       0.91      0.84      0.88        25\n",
      "  Paramedics       0.92      0.92      0.92        25\n",
      "      Person       0.96      0.96      0.96        25\n",
      "      Police       0.96      0.92      0.94        25\n",
      "  Road_block       0.82      0.72      0.77        25\n",
      "     Warning       0.96      0.96      0.96        25\n",
      "\n",
      "    accuracy                           0.90       350\n",
      "   macro avg       0.91      0.90      0.90       350\n",
      "weighted avg       0.91      0.90      0.90       350\n",
      "\n",
      "Classes found: ['Bomb', 'Car', 'Casualty', 'Electricity', 'Fire', 'Fire_brigade', 'Flood', 'Gas', 'Injury', 'Paramedics', 'Person', 'Police', 'Road_block', 'Warning']\n",
      "Total participants: 30\n",
      "Training participants (21): ['27', '22', '19', '6', '3']...\n",
      "Testing participants (5): ['17', '30', '1', '12', '28']...\n",
      "Sample Sizes -> Train: 1465, Val: 280, Test: 350\n",
      "\n",
      "------------------------------\n",
      "NOW TRAINING simple_cnn\n",
      "\n",
      "------------------------------\n",
      "Epoch 1/10 | Train Loss: 1.7662 | Train Acc: 0.5133 | Val Loss: 0.4492 | Val Acc: 0.8893\n",
      "Epoch 2/10 | Train Loss: 0.7529 | Train Acc: 0.7597 | Val Loss: 0.3459 | Val Acc: 0.8857\n",
      "Epoch 3/10 | Train Loss: 0.5508 | Train Acc: 0.8218 | Val Loss: 0.2324 | Val Acc: 0.9393\n",
      "Epoch 4/10 | Train Loss: 0.4514 | Train Acc: 0.8683 | Val Loss: 0.1331 | Val Acc: 0.9643\n",
      "Epoch 5/10 | Train Loss: 0.3809 | Train Acc: 0.8853 | Val Loss: 0.1389 | Val Acc: 0.9536\n",
      "Epoch 6/10 | Train Loss: 0.2566 | Train Acc: 0.9195 | Val Loss: 0.1121 | Val Acc: 0.9679\n",
      "Epoch 7/10 | Train Loss: 0.2358 | Train Acc: 0.9290 | Val Loss: 0.1241 | Val Acc: 0.9643\n",
      "Epoch 8/10 | Train Loss: 0.2096 | Train Acc: 0.9406 | Val Loss: 0.0948 | Val Acc: 0.9750\n",
      "Epoch 9/10 | Train Loss: 0.1556 | Train Acc: 0.9488 | Val Loss: 0.0981 | Val Acc: 0.9679\n",
      "Epoch 10/10 | Train Loss: 0.1254 | Train Acc: 0.9659 | Val Loss: 0.0958 | Val Acc: 0.9679\n",
      "\n",
      "--- simple_cnn TEST RESULTS ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bomb       0.96      1.00      0.98        25\n",
      "         Car       0.96      1.00      0.98        25\n",
      "    Casualty       0.93      1.00      0.96        25\n",
      " Electricity       1.00      1.00      1.00        25\n",
      "        Fire       0.96      0.92      0.94        25\n",
      "Fire_brigade       0.96      1.00      0.98        25\n",
      "       Flood       1.00      0.84      0.91        25\n",
      "         Gas       0.92      0.92      0.92        25\n",
      "      Injury       0.96      0.96      0.96        25\n",
      "  Paramedics       0.96      1.00      0.98        25\n",
      "      Person       1.00      1.00      1.00        25\n",
      "      Police       0.96      0.92      0.94        25\n",
      "  Road_block       0.96      0.96      0.96        25\n",
      "     Warning       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           0.97       350\n",
      "   macro avg       0.97      0.97      0.97       350\n",
      "weighted avg       0.97      0.97      0.97       350\n",
      "\n",
      "Model Saved\n",
      "\n",
      "------------------------------\n",
      "NOW TRAINING resnet50\n",
      "\n",
      "------------------------------\n",
      "Epoch 1/10 | Train Loss: 1.7569 | Train Acc: 0.6184 | Val Loss: 0.5288 | Val Acc: 0.8964\n",
      "Epoch 2/10 | Train Loss: 0.1492 | Train Acc: 0.9782 | Val Loss: 0.0241 | Val Acc: 0.9964\n",
      "Epoch 3/10 | Train Loss: 0.0536 | Train Acc: 0.9870 | Val Loss: 0.0259 | Val Acc: 0.9857\n",
      "Epoch 4/10 | Train Loss: 0.0401 | Train Acc: 0.9918 | Val Loss: 0.0111 | Val Acc: 1.0000\n",
      "Epoch 5/10 | Train Loss: 0.0332 | Train Acc: 0.9918 | Val Loss: 0.0139 | Val Acc: 0.9964\n",
      "Epoch 6/10 | Train Loss: 0.0208 | Train Acc: 0.9959 | Val Loss: 0.0052 | Val Acc: 1.0000\n",
      "Epoch 7/10 | Train Loss: 0.0249 | Train Acc: 0.9932 | Val Loss: 0.0075 | Val Acc: 0.9964\n",
      "Epoch 8/10 | Train Loss: 0.0172 | Train Acc: 0.9945 | Val Loss: 0.0319 | Val Acc: 0.9893\n",
      "Epoch 9/10 | Train Loss: 0.0177 | Train Acc: 0.9966 | Val Loss: 0.0166 | Val Acc: 0.9964\n",
      "Epoch 10/10 | Train Loss: 0.0153 | Train Acc: 0.9959 | Val Loss: 0.0038 | Val Acc: 1.0000\n",
      "\n",
      "--- resnet50 TEST RESULTS ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bomb       1.00      1.00      1.00        25\n",
      "         Car       1.00      1.00      1.00        25\n",
      "    Casualty       1.00      1.00      1.00        25\n",
      " Electricity       0.96      1.00      0.98        25\n",
      "        Fire       1.00      1.00      1.00        25\n",
      "Fire_brigade       1.00      0.96      0.98        25\n",
      "       Flood       1.00      1.00      1.00        25\n",
      "         Gas       1.00      1.00      1.00        25\n",
      "      Injury       1.00      1.00      1.00        25\n",
      "  Paramedics       1.00      1.00      1.00        25\n",
      "      Person       1.00      1.00      1.00        25\n",
      "      Police       1.00      1.00      1.00        25\n",
      "  Road_block       1.00      1.00      1.00        25\n",
      "     Warning       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           1.00       350\n",
      "   macro avg       1.00      1.00      1.00       350\n",
      "weighted avg       1.00      1.00      1.00       350\n",
      "\n",
      "Model Saved\n",
      "\n",
      "------------------------------\n",
      "NOW TRAINING vit_b_16\n",
      "\n",
      "------------------------------\n",
      "Epoch 1/10 | Train Loss: 0.4886 | Train Acc: 0.8840 | Val Loss: 0.0412 | Val Acc: 0.9929\n",
      "Epoch 2/10 | Train Loss: 0.0666 | Train Acc: 0.9843 | Val Loss: 0.0878 | Val Acc: 0.9679\n",
      "Epoch 3/10 | Train Loss: 0.0263 | Train Acc: 0.9952 | Val Loss: 0.0080 | Val Acc: 1.0000\n",
      "Epoch 4/10 | Train Loss: 0.0668 | Train Acc: 0.9836 | Val Loss: 0.0540 | Val Acc: 0.9821\n",
      "Epoch 5/10 | Train Loss: 0.1183 | Train Acc: 0.9645 | Val Loss: 0.7655 | Val Acc: 0.7750\n",
      "Epoch 6/10 | Train Loss: 0.1191 | Train Acc: 0.9631 | Val Loss: 0.0532 | Val Acc: 0.9893\n",
      "Epoch 7/10 | Train Loss: 0.0568 | Train Acc: 0.9816 | Val Loss: 0.0057 | Val Acc: 1.0000\n",
      "Epoch 8/10 | Train Loss: 0.0584 | Train Acc: 0.9795 | Val Loss: 0.1414 | Val Acc: 0.9607\n",
      "Epoch 9/10 | Train Loss: 0.0814 | Train Acc: 0.9679 | Val Loss: 0.1232 | Val Acc: 0.9536\n",
      "Epoch 10/10 | Train Loss: 0.0446 | Train Acc: 0.9870 | Val Loss: 0.0116 | Val Acc: 0.9964\n",
      "\n",
      "--- vit_b_16 TEST RESULTS ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bomb       0.96      1.00      0.98        25\n",
      "         Car       1.00      1.00      1.00        25\n",
      "    Casualty       1.00      0.96      0.98        25\n",
      " Electricity       0.96      1.00      0.98        25\n",
      "        Fire       0.96      0.96      0.96        25\n",
      "Fire_brigade       1.00      0.96      0.98        25\n",
      "       Flood       1.00      1.00      1.00        25\n",
      "         Gas       1.00      0.96      0.98        25\n",
      "      Injury       1.00      1.00      1.00        25\n",
      "  Paramedics       1.00      1.00      1.00        25\n",
      "      Person       0.96      1.00      0.98        25\n",
      "      Police       1.00      1.00      1.00        25\n",
      "  Road_block       1.00      1.00      1.00        25\n",
      "     Warning       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           0.99       350\n",
      "   macro avg       0.99      0.99      0.99       350\n",
      "weighted avg       0.99      0.99      0.99       350\n",
      "\n",
      "Model Saved\n",
      "\n",
      "------------------------------\n",
      "NOW TRAINING resnet18\n",
      "\n",
      "------------------------------\n",
      "Epoch 1/10 | Train Loss: 0.6071 | Train Acc: 0.8751 | Val Loss: 0.0385 | Val Acc: 1.0000\n",
      "Epoch 2/10 | Train Loss: 0.0572 | Train Acc: 0.9945 | Val Loss: 0.0277 | Val Acc: 0.9964\n",
      "Epoch 3/10 | Train Loss: 0.0279 | Train Acc: 0.9980 | Val Loss: 0.0105 | Val Acc: 1.0000\n",
      "Epoch 4/10 | Train Loss: 0.0201 | Train Acc: 0.9973 | Val Loss: 0.0059 | Val Acc: 1.0000\n",
      "Epoch 5/10 | Train Loss: 0.0189 | Train Acc: 0.9966 | Val Loss: 0.0110 | Val Acc: 1.0000\n",
      "Epoch 6/10 | Train Loss: 0.0151 | Train Acc: 0.9986 | Val Loss: 0.0048 | Val Acc: 1.0000\n",
      "Epoch 7/10 | Train Loss: 0.0103 | Train Acc: 0.9986 | Val Loss: 0.0061 | Val Acc: 1.0000\n",
      "Epoch 8/10 | Train Loss: 0.0116 | Train Acc: 0.9980 | Val Loss: 0.0018 | Val Acc: 1.0000\n",
      "Epoch 9/10 | Train Loss: 0.0074 | Train Acc: 0.9993 | Val Loss: 0.0034 | Val Acc: 1.0000\n",
      "Epoch 10/10 | Train Loss: 0.0115 | Train Acc: 0.9986 | Val Loss: 0.0033 | Val Acc: 1.0000\n",
      "\n",
      "--- resnet18 TEST RESULTS ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bomb       1.00      1.00      1.00        25\n",
      "         Car       1.00      1.00      1.00        25\n",
      "    Casualty       1.00      1.00      1.00        25\n",
      " Electricity       1.00      1.00      1.00        25\n",
      "        Fire       1.00      1.00      1.00        25\n",
      "Fire_brigade       1.00      0.96      0.98        25\n",
      "       Flood       1.00      1.00      1.00        25\n",
      "         Gas       1.00      1.00      1.00        25\n",
      "      Injury       1.00      1.00      1.00        25\n",
      "  Paramedics       0.96      1.00      0.98        25\n",
      "      Person       1.00      1.00      1.00        25\n",
      "      Police       1.00      1.00      1.00        25\n",
      "  Road_block       1.00      1.00      1.00        25\n",
      "     Warning       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           1.00       350\n",
      "   macro avg       1.00      1.00      1.00       350\n",
      "weighted avg       1.00      1.00      1.00       350\n",
      "\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"../data/extracted\"\n",
    "JSON_FEATURES_PATH = \"../features_dataset.json\" \n",
    "\n",
    "MODELS_DL = [\"simple_cnn\", \"resnet50\", \"vit_b_16\", \"resnet18\"]\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.0001\n",
    "\n",
    "def main_pipeline():\n",
    "    print(f\"Working on {DEVICE}\")\n",
    "\n",
    "    #  TRADITIONAL ML PART\n",
    "\n",
    "    if Path(JSON_FEATURES_PATH).exists():\n",
    "        train_ml(JSON_FEATURES_PATH)\n",
    "    else:\n",
    "        print(f\"Warning: {JSON_FEATURES_PATH} not found. Skipping ML training.\")\n",
    "        print(\"Please run extraction.ipynb first to generate the JSON file.\")\n",
    "    \n",
    "    #  DEEP LEARNING PART\n",
    "\n",
    "    if not Path(DATA_PATH).exists():\n",
    "        print(f\"Error: {DATA_PATH} not found.\")\n",
    "        return\n",
    "\n",
    "    # Using create_dataloaders function defined in previous cells\n",
    "    train_loader, val_loader, test_loader, class_names = create_dataloaders(DATA_PATH, BATCH_SIZE)\n",
    "\n",
    "    if not os.path.exists(\"models\"):\n",
    "        os.makedirs(\"models\")\n",
    "\n",
    "    for model_name in MODELS_DL:\n",
    "        print(f\"\\n{'-'*30}\")\n",
    "        print(f\"NOW TRAINING {model_name}\")\n",
    "        print(f\"\\n{'-'*30}\")\n",
    "\n",
    "        model = get_model(model_name, len(class_names), DEVICE)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "        results = {\n",
    "            \"train_loss\": [],\n",
    "            \"train_acc\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"val_acc\": []\n",
    "        }\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            # train and validate functions \n",
    "            train_loss, train_acc = train(model, train_loader, criterion, optimizer, DEVICE)\n",
    "            val_loss, val_acc = validate(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "            t_acc = train_acc.item() if isinstance(train_acc, torch.Tensor) else train_acc\n",
    "            v_acc = val_acc.item() if isinstance(val_acc, torch.Tensor) else val_acc\n",
    "\n",
    "            results[\"train_loss\"].append(train_loss)\n",
    "            results[\"train_acc\"].append(t_acc)\n",
    "            results[\"val_loss\"].append(val_loss)\n",
    "            results[\"val_acc\"].append(v_acc)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "                f\"Train Loss: {train_loss:.4f} | \"\n",
    "                f\"Train Acc: {t_acc:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} | \"\n",
    "                f\"Val Acc: {v_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "        print(f\"\\n--- {model_name} TEST RESULTS ---\")\n",
    "\n",
    "        # test\n",
    "        y_true, y_pred = test(model, test_loader, DEVICE, class_names)\n",
    "\n",
    "        # Visualization\n",
    "        plot_curves(results, model_name)\n",
    "        plot_confusion_matrix(y_true, y_pred, class_names, model_name)\n",
    "\n",
    "        # Saving the Model\n",
    "        torch.save(model.state_dict(), f\"models/{model_name}_final.pth\")\n",
    "        print(\"Model Saved\")\n",
    "\n",
    "        # Memory cleanup\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
